import { useState, useEffect, useRef, useMemo, useCallback } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { AudioRecorder } from '@/lib/audio-recorder';
import { AudioStreamer } from '@/lib/audio-streamer';
import { audioContext } from '@/lib/utils';
import { LiveConnectConfig } from "@google/genai";

export type UseOpenAIRealtimeResults = {
  client: any;
  agent: null;
  setConfig: (config: LiveConnectConfig | { instructions?: string; tools?: any[] }) => void;
  config: { instructions?: string; tools?: any[] };
  model: string;
  setModel: (model: string) => void;
  connected: boolean;
  setupComplete: boolean;
  connect: () => Promise<void>;
  disconnect: () => Promise<void>;
  volume: number;
  send: (parts: Array<{ text: string }>) => void;
  sendToolResponse: (response: any) => void;
  on: (event: string, handler: any) => void;
  off: (event: string, handler: any) => void;
};

// Gemini Type enumì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
function convertGeminiTypeToOpenAI(type: any): string {
  if (typeof type === 'string') {
    return type.toLowerCase();
  }
  const typeStr = String(type);
  return typeStr.toLowerCase();
}

// Gemini parametersë¥¼ OpenAI í˜•ì‹ìœ¼ë¡œ ì¬ê·€ì ìœ¼ë¡œ ë³€í™˜
function convertParameters(params: any): any {
  if (!params || typeof params !== 'object') {
    return params;
  }

  const converted: any = {};
  
  for (const key in params) {
    if (key === 'type') {
      converted.type = convertGeminiTypeToOpenAI(params.type);
    } else if (key === 'properties' && typeof params.properties === 'object') {
      converted.properties = {};
      for (const propKey in params.properties) {
        converted.properties[propKey] = convertParameters(params.properties[propKey]);
      }
    } else if (key === 'items' && typeof params.items === 'object') {
      converted.items = convertParameters(params.items);
    } else if (key === 'enum' && Array.isArray(params.enum)) {
      converted.enum = params.enum;
    } else if (key === 'description') {
      converted.description = params.description;
    } else if (key === 'required' && Array.isArray(params.required)) {
      converted.required = params.required;
    }
  }
  
  return converted;
}

// Gemini configë¥¼ OpenAI instructionsë¡œ ë³€í™˜
function convertGeminiConfigToOpenAI(config: LiveConnectConfig): { instructions: string; tools: any[] } {
  let instructions = '';
  let tools: any[] = [];

  const systemInst = config.systemInstruction as any;
  if (systemInst?.parts) {
    instructions = systemInst.parts
      .map((part: any) => {
        if (typeof part === 'string') return part;
        if (part?.text) return part.text;
        return '';
      })
      .filter(Boolean)
      .join('\n');
  }

  // Gemini toolsë¥¼ OpenAI Realtime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
  if (config.tools && Array.isArray(config.tools)) {
    tools = config.tools
      .filter((tool: any) => tool?.functionDeclarations)
      .flatMap((tool: any) => 
        tool.functionDeclarations.map((func: any) => ({
          type: 'function',
          name: func.name,
          description: func.description,
          parameters: convertParameters(func.parameters)
        }))
      );
  }

  console.log('âœ… [OpenAI] Converted config:', {
    instructionsLength: instructions.length,
    toolsCount: tools.length
  });

  return { instructions, tools };
}

export function useOpenAIRealtime(): UseOpenAIRealtimeResults {
  const [model, setModel] = useState('gpt-4o-realtime-preview-2024-12-17');
  const [connected, setConnected] = useState(false);
  const [setupComplete, setSetupComplete] = useState(false);
  const [volume, setVolume] = useState(0);
  const [config, setConfigState] = useState<{ instructions?: string; tools?: any[] }>({});

  const wsRef = useRef<WebSocket | null>(null);
  const audioRecorderRef = useRef<AudioRecorder | null>(null);
  const audioStreamerRef = useRef<AudioStreamer | null>(null);
  const eventHandlersRef = useRef<Map<string, Set<Function>>>(new Map());
  const ephemeralKeyRef = useRef<string>('');
  const sessionCreatedRef = useRef(false);
  const connectingRef = useRef(false);

  // Ephemeral key ìƒì„±
  const generateEphemeralKey = async (): Promise<string> => {
    try {
      console.log('ğŸ”‘ [OpenAI] Generating ephemeral key...');
      const { data, error } = await supabase.functions.invoke('realtime-client-secret');
      
      if (error) throw error;
      
      const clientSecret = data?.clientSecret;
      if (!clientSecret) throw new Error('No client secret received');
      
      ephemeralKeyRef.current = clientSecret;
      console.log('âœ… [OpenAI] Ephemeral key generated');
      return clientSecret;
    } catch (error) {
      console.error('âŒ [OpenAI] Failed to generate ephemeral key:', error);
      throw error;
    }
  };

  // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ê´€ë¦¬
  const triggerEvent = (event: string, data: any) => {
    const handlers = eventHandlersRef.current.get(event);
    if (handlers) {
      handlers.forEach(handler => handler(data));
    }
  };

  const on = useCallback((event: string, handler: Function) => {
    if (!eventHandlersRef.current.has(event)) {
      eventHandlersRef.current.set(event, new Set());
    }
    eventHandlersRef.current.get(event)?.add(handler);
  }, []);

  const off = useCallback((event: string, handler: Function) => {
    eventHandlersRef.current.get(event)?.delete(handler);
  }, []);

  // ì„¤ì • ì—…ë°ì´íŠ¸
  const setConfig = useCallback((newConfig: LiveConnectConfig | { instructions?: string; tools?: any[] }) => {
    console.log('ğŸ“¥ [OpenAI] setConfig called');
    
    // Gemini í˜•ì‹ì˜ configì¸ì§€ í™•ì¸
    if ('systemInstruction' in newConfig) {
      const converted = convertGeminiConfigToOpenAI(newConfig as LiveConnectConfig);
      setConfigState(converted);
      setSetupComplete(true);
      console.log('âœ… [OpenAI] Config converted and set');
    } else {
      setConfigState(newConfig);
      setSetupComplete(true);
      console.log('âœ… [OpenAI] Config set directly');
    }
  }, []);

  // ì—°ê²°
  const connect = useCallback(async () => {
    if (connectingRef.current || connected) {
      console.log('â¸ï¸ [OpenAI] Already connecting or connected');
      return;
    }

    if (!config.instructions) {
      console.error('âŒ [OpenAI] No instructions configured');
      return;
    }

    connectingRef.current = true;

    try {
      console.log('ğŸ”Œ [OpenAI] Starting WebSocket connection...');
      
      const clientSecret = await generateEphemeralKey();
      const url = `wss://api.openai.com/v1/realtime?model=${model}`;
      
      wsRef.current = new WebSocket(url, [
        'realtime',
        `openai-insecure-api-key.${clientSecret}`
      ]);

      wsRef.current.onopen = () => {
        console.log('âœ… [OpenAI] WebSocket connected');
        sessionCreatedRef.current = false;
      };

      wsRef.current.onmessage = async (event) => {
        try {
          const data = JSON.parse(event.data);
          console.log('ğŸ“¨ [OpenAI] Received:', data.type);

          // session.created ì´ë²¤íŠ¸ë¥¼ ë°›ìœ¼ë©´ session.update ì „ì†¡
          if (data.type === 'session.created' && !sessionCreatedRef.current) {
            sessionCreatedRef.current = true;
            console.log('ğŸ”§ [OpenAI] Sending session.update...');
            
            wsRef.current?.send(JSON.stringify({
              type: 'session.update',
              session: {
                type: 'realtime',
                model: 'gpt-4o-realtime-preview-2024-12-17',
                output_modalities: ['audio'],
                audio: {
                  input: {
                    format: {
                      type: 'audio/pcm',
                      rate: 24000
                    },
                    turn_detection: {
                      type: 'semantic_vad'
                    }
                  },
                  output: {
                    format: {
                      type: 'audio/pcm',
                      rate: 24000
                    },
                    voice: 'alloy'
                  }
                },
                instructions: config.instructions,
                tools: config.tools || [],
                tool_choice: 'auto'
              }
            }));
          }

          // session.updated ì´ë²¤íŠ¸
          if (data.type === 'session.updated') {
            console.log('âœ… [OpenAI] Session updated successfully');
            setConnected(true);
            triggerEvent('setupcomplete', {});
            // Server VAD ëª¨ë“œì—ì„œëŠ” ìë™ìœ¼ë¡œ ì‘ë‹µ ìƒì„±ë˜ë¯€ë¡œ ìˆ˜ë™ íŠ¸ë¦¬ê±° ë¶ˆí•„ìš”
          }

          // ì˜¤ë””ì˜¤ ì‘ë‹µ ì²˜ë¦¬
          if (data.type === 'response.audio.delta' && data.delta) {
            const binaryString = atob(data.delta);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
              bytes[i] = binaryString.charCodeAt(i);
            }
            
            if (!audioStreamerRef.current) {
              audioStreamerRef.current = new AudioStreamer(await audioContext());
            }
            
            audioStreamerRef.current.addPCM16(bytes);
          }

          // í…ìŠ¤íŠ¸ ì‘ë‹µ ì²˜ë¦¬
          if (data.type === 'response.audio_transcript.delta' && data.delta) {
            triggerEvent('message', {
              type: 'transcription',
              text: data.delta
            });
          }

          // Tool call ì²˜ë¦¬
          if (data.type === 'response.function_call_arguments.done') {
            triggerEvent('toolcall', {
              functionCalls: [{
                name: data.name,
                args: JSON.parse(data.arguments)
              }]
            });
          }

          // ì‘ë‹µ ì™„ë£Œ
          if (data.type === 'response.done') {
            triggerEvent('turncomplete', {});
          }

          // ì—ëŸ¬ ì²˜ë¦¬
          if (data.type === 'error') {
            console.error('âŒ [OpenAI] Error:', data.error);
            triggerEvent('error', data.error);
          }
        } catch (error) {
          console.error('âŒ [OpenAI] Message parsing error:', error);
        }
      };

      wsRef.current.onerror = (error) => {
        console.error('âŒ [OpenAI] WebSocket error:', error);
        setConnected(false);
      };

      wsRef.current.onclose = () => {
        console.log('ğŸ”Œ [OpenAI] WebSocket closed');
        setConnected(false);
        sessionCreatedRef.current = false;
      };

      // ì˜¤ë””ì˜¤ ë…¹ìŒ ì‹œì‘
      if (!audioRecorderRef.current) {
        console.log('ğŸ¤ [OpenAI] Starting audio recorder...');
        audioRecorderRef.current = new AudioRecorder(24000);
        audioRecorderRef.current.on('data', (base64Audio: string) => {
          if (wsRef.current?.readyState === WebSocket.OPEN) {
            wsRef.current.send(JSON.stringify({
              type: 'input_audio_buffer.append',
              audio: base64Audio
            }));
          }
        });
        await audioRecorderRef.current.start();
        console.log('âœ… [OpenAI] Audio recorder started');
      }
    } catch (error) {
      console.error('âŒ [OpenAI] Connection error:', error);
      setConnected(false);
    } finally {
      connectingRef.current = false;
    }
  }, [config, model]);

  // ì—°ê²° í•´ì œ
  const disconnect = useCallback(async () => {
    console.log('ğŸ”Œ [OpenAI] Disconnecting...');
    
    if (audioRecorderRef.current) {
      audioRecorderRef.current.stop();
      audioRecorderRef.current = null;
    }

    if (audioStreamerRef.current) {
      audioStreamerRef.current.stop();
      audioStreamerRef.current = null;
    }

    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }

    setConnected(false);
    sessionCreatedRef.current = false;
    console.log('âœ… [OpenAI] Disconnected');
  }, []);

  // ë©”ì‹œì§€ ì „ì†¡
  const send = useCallback((parts: Array<{ text: string }>) => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      console.warn('âš ï¸ [OpenAI] WebSocket not ready');
      return;
    }

    const text = parts.map(p => p.text).join(' ');
    console.log('ğŸ“¤ [OpenAI] Sending message:', text.substring(0, 100));

    wsRef.current.send(JSON.stringify({
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'user',
        content: [{
          type: 'input_text',
          text: text
        }]
      }
    }));

    // Server VAD ëª¨ë“œì—ì„œëŠ” ì˜¤ë””ì˜¤ ì…ë ¥ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬
    // í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë„ Server VADê°€ ì²˜ë¦¬í•˜ë„ë¡ ëŒ€ê¸°
    console.log('âœ… [OpenAI] Message sent, Server VAD will handle response');
  }, []);

  // Tool ì‘ë‹µ ì „ì†¡
  const sendToolResponse = useCallback((response: any) => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;
    
    console.log('ğŸ”§ [OpenAI] Sending tool response:', response);
    wsRef.current.send(JSON.stringify({
      type: 'conversation.item.create',
      item: {
        type: 'function_call_output',
        call_id: response.id,
        output: JSON.stringify(response.result)
      }
    }));
  }, []);

  // í´ë¼ì´ì–¸íŠ¸ ë˜í¼
  const clientWrapper = useMemo(() => ({
    send,
    sendToolResponse,
    sendRealtimeInput: (chunks: Array<{ mimeType: string; data: string }>) => {
      if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;
      
      chunks.forEach((chunk) => {
        if (chunk.mimeType.includes('audio')) {
          wsRef.current?.send(JSON.stringify({
            type: 'input_audio_buffer.append',
            audio: chunk.data
          }));
        }
      });
    }
  }), [send, sendToolResponse]);

  // Cleanup
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    client: clientWrapper,
    agent: null,
    setConfig,
    config,
    model,
    setModel,
    connected,
    setupComplete,
    connect,
    disconnect,
    volume,
    send,
    sendToolResponse,
    on,
    off
  };
}
