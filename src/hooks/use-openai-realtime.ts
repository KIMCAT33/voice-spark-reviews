import { useState, useEffect, useRef, useMemo, useCallback } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { AudioRecorder } from '@/lib/audio-recorder';
import { AudioStreamer } from '@/lib/audio-streamer';
import { audioContext } from '@/lib/utils';
import { LiveConnectConfig } from "@google/genai";

export type UseOpenAIRealtimeResults = {
  client: any;
  agent: null;
  setConfig: (config: LiveConnectConfig | { instructions?: string; tools?: any[] }) => void;
  config: { instructions?: string; tools?: any[] };
  model: string;
  setModel: (model: string) => void;
  connected: boolean;
  setupComplete: boolean;
  connect: () => Promise<void>;
  disconnect: () => Promise<void>;
  volume: number;
  send: (parts: Array<{ text: string }>) => void;
  sendToolResponse: (response: any) => void;
  on: (event: string, handler: any) => void;
  off: (event: string, handler: any) => void;
};

// Gemini Type enumì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
function convertGeminiTypeToOpenAI(type: any): string {
  if (typeof type === 'string') {
    return type.toLowerCase();
  }
  const typeStr = String(type);
  return typeStr.toLowerCase();
}

// Gemini parametersë¥¼ OpenAI í˜•ì‹ìœ¼ë¡œ ì¬ê·€ì ìœ¼ë¡œ ë³€í™˜
function convertParameters(params: any): any {
  if (!params || typeof params !== 'object') {
    return params;
  }

  const converted: any = {};
  
  for (const key in params) {
    if (key === 'type') {
      converted.type = convertGeminiTypeToOpenAI(params.type);
    } else if (key === 'properties' && typeof params.properties === 'object') {
      converted.properties = {};
      for (const propKey in params.properties) {
        converted.properties[propKey] = convertParameters(params.properties[propKey]);
      }
    } else if (key === 'items' && typeof params.items === 'object') {
      converted.items = convertParameters(params.items);
    } else if (key === 'enum' && Array.isArray(params.enum)) {
      converted.enum = params.enum;
    } else if (key === 'description') {
      converted.description = params.description;
    } else if (key === 'required' && Array.isArray(params.required)) {
      converted.required = params.required;
    }
  }
  
  return converted;
}

// Gemini configë¥¼ OpenAI instructionsë¡œ ë³€í™˜
function convertGeminiConfigToOpenAI(config: LiveConnectConfig): { instructions: string; tools: any[] } {
  let instructions = '';
  let tools: any[] = [];

  const systemInst = config.systemInstruction as any;
  if (systemInst?.parts) {
    instructions = systemInst.parts
      .map((part: any) => {
        if (typeof part === 'string') return part;
        if (part?.text) return part.text;
        return '';
      })
      .filter(Boolean)
      .join('\n');
  }

  // Gemini toolsë¥¼ OpenAI Realtime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
  if (config.tools && Array.isArray(config.tools)) {
    tools = config.tools
      .filter((tool: any) => tool?.functionDeclarations)
      .flatMap((tool: any) => 
        tool.functionDeclarations.map((func: any) => ({
          type: 'function',
          name: func.name,
          description: func.description,
          parameters: convertParameters(func.parameters)
        }))
      );
  }

  console.log('âœ… [OpenAI] Converted config:', {
    instructionsLength: instructions.length,
    toolsCount: tools.length
  });

  return { instructions, tools };
}

export function useOpenAIRealtime(): UseOpenAIRealtimeResults {
  const [model, setModel] = useState('gpt-4o-realtime-preview-2024-12-17');
  const [connected, setConnected] = useState(false);
  const [setupComplete, setSetupComplete] = useState(false);
  const [volume, setVolume] = useState(0);
  const [config, setConfigState] = useState<{ instructions?: string; tools?: any[] }>({});

  const wsRef = useRef<WebSocket | null>(null);
  const audioRecorderRef = useRef<AudioRecorder | null>(null);
  const audioStreamerRef = useRef<AudioStreamer | null>(null);
  const eventHandlersRef = useRef<Map<string, Set<Function>>>(new Map());
  const ephemeralKeyRef = useRef<string>('');
  const sessionCreatedRef = useRef(false);
  const connectingRef = useRef(false);

  // Ephemeral key ìƒì„±
  const generateEphemeralKey = async (): Promise<string> => {
    try {
      console.log('ğŸ”‘ [OpenAI] Generating ephemeral key...');
      const { data, error } = await supabase.functions.invoke('realtime-client-secret');
      
      if (error) throw error;
      
      const clientSecret = data?.clientSecret;
      if (!clientSecret) throw new Error('No client secret received');
      
      ephemeralKeyRef.current = clientSecret;
      console.log('âœ… [OpenAI] Ephemeral key generated');
      return clientSecret;
    } catch (error) {
      console.error('âŒ [OpenAI] Failed to generate ephemeral key:', error);
      throw error;
    }
  };

  // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ê´€ë¦¬
  const triggerEvent = (event: string, data: any) => {
    const handlers = eventHandlersRef.current.get(event);
    if (handlers) {
      handlers.forEach(handler => handler(data));
    }
  };

  const on = useCallback((event: string, handler: Function) => {
    if (!eventHandlersRef.current.has(event)) {
      eventHandlersRef.current.set(event, new Set());
    }
    eventHandlersRef.current.get(event)?.add(handler);
  }, []);

  const off = useCallback((event: string, handler: Function) => {
    eventHandlersRef.current.get(event)?.delete(handler);
  }, []);

  // ì„¤ì • ì—…ë°ì´íŠ¸
  const setConfig = useCallback((newConfig: LiveConnectConfig | { instructions?: string; tools?: any[] }) => {
    console.log('ğŸ“¥ [OpenAI] setConfig called');
    
    // Gemini í˜•ì‹ì˜ configì¸ì§€ í™•ì¸
    if ('systemInstruction' in newConfig) {
      const converted = convertGeminiConfigToOpenAI(newConfig as LiveConnectConfig);
      setConfigState(converted);
      setSetupComplete(true);
      console.log('âœ… [OpenAI] Config converted and set');
    } else {
      setConfigState(newConfig);
      setSetupComplete(true);
      console.log('âœ… [OpenAI] Config set directly');
    }
  }, []);

  // ì—°ê²°
  const connect = useCallback(async () => {
    if (connectingRef.current || connected) {
      console.log('â¸ï¸ [OpenAI] Already connecting or connected');
      return;
    }

    if (!config.instructions) {
      console.error('âŒ [OpenAI] No instructions configured');
      return;
    }

    connectingRef.current = true;

    try {
      console.log('ğŸ”Œ [OpenAI] Starting WebSocket connection...');
      
      const clientSecret = await generateEphemeralKey();
      const url = `wss://api.openai.com/v1/realtime?model=${model}`;
      
      wsRef.current = new WebSocket(url, [
        'realtime',
        `openai-insecure-api-key.${clientSecret}`
      ]);

      wsRef.current.onopen = () => {
        console.log('âœ… [OpenAI] WebSocket connected');
        sessionCreatedRef.current = false;
      };

      wsRef.current.onmessage = async (event) => {
        try {
          const data = JSON.parse(event.data);
          // ëª¨ë“  ì´ë²¤íŠ¸ íƒ€ì… ë¡œê¹… (ë””ë²„ê¹…ìš©)
          // response ê´€ë ¨ ì´ë²¤íŠ¸ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ë¡œê¹…
          if (data.type) {
            if (data.type.includes('response') || data.type.includes('audio')) {
              console.log('ğŸ“¨ [OpenAI] Received:', data.type, JSON.stringify(data, null, 2));
            } else {
              console.log('ğŸ“¨ [OpenAI] Received:', data.type);
            }
          } else {
            console.log('ğŸ“¨ [OpenAI] Received (no type):', Object.keys(data).slice(0, 5));
          }

          // session.created ì´ë²¤íŠ¸ë¥¼ ë°›ìœ¼ë©´ session.update ì „ì†¡
          if (data.type === 'session.created' && !sessionCreatedRef.current) {
            sessionCreatedRef.current = true;
            console.log('ğŸ”§ [OpenAI] Sending session.update...');
            
            wsRef.current?.send(JSON.stringify({
              type: 'session.update',
              session: {
                type: 'realtime',
                model: 'gpt-4o-realtime-preview-2024-12-17',
                output_modalities: ['audio'],
                audio: {
                  input: {
                    format: {
                      type: 'audio/pcm',
                      rate: 24000
                    },
                    turn_detection: {
                      type: 'semantic_vad'
                    }
                  },
                  output: {
                    format: {
                      type: 'audio/pcm',
                      rate: 24000
                    },
                    voice: 'alloy'
                  }
                },
                instructions: config.instructions,
                tools: config.tools || [],
                tool_choice: 'auto'
              }
            }));
          }

          // session.updated ì´ë²¤íŠ¸
          if (data.type === 'session.updated') {
            console.log('âœ… [OpenAI] Session updated successfully');
            setConnected(true);
            triggerEvent('setupcomplete', {});
            
            // OpenAI Realtime APIëŠ” ì´ˆê¸° ì‘ë‹µì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­
            // ì •ìƒ ì‘ë™í•˜ëŠ” í”„ë¡œì íŠ¸ ë°©ì‹: conversation.item.createë¡œ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¨¼ì € ë³´ë‚´ê³ 
            // ê·¸ ë‹¤ìŒ response.createë¥¼ í˜¸ì¶œí•˜ë©´ ì‘ë‹µì´ ìƒì„±ë¨
            const requestInitialResponse = () => {
              if (wsRef.current?.readyState === WebSocket.OPEN && audioRecorderRef.current) {
                console.log('ğŸš€ [OpenAI] Requesting initial response after audio recorder is ready...');
                try {
                  // 1ë‹¨ê³„: conversation.item.createë¡œ ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„± (íŠ¸ë¦¬ê±°ìš©)
                  wsRef.current.send(JSON.stringify({
                    type: 'conversation.item.create',
                    item: {
                      type: 'message',
                      role: 'user',
                      content: [{
                        type: 'input_text',
                        text: 'Hello'
                      }]
                    }
                  }));
                  console.log('âœ… [OpenAI] conversation.item.create sent (trigger message)');
                  
                  // 2ë‹¨ê³„: response.createë¡œ AI ì‘ë‹µ íŠ¸ë¦¬ê±° (ì§§ì€ ë”œë ˆì´)
                  setTimeout(() => {
                    if (wsRef.current?.readyState === WebSocket.OPEN) {
                      wsRef.current.send(JSON.stringify({
                        type: 'response.create'
                      }));
                      console.log('âœ… [OpenAI] response.create sent - AI should respond now');
                    } else {
                      console.warn('âš ï¸ [OpenAI] WebSocket closed before response.create');
                    }
                  }, 300);
                } catch (error) {
                  console.error('âŒ [OpenAI] Failed to send initial response request:', error);
                }
              } else {
                if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
                  console.warn('âš ï¸ [OpenAI] WebSocket not OPEN yet, retrying...', wsRef.current?.readyState);
                }
                if (!audioRecorderRef.current) {
                  console.warn('âš ï¸ [OpenAI] Audio recorder not ready yet, retrying...');
                }
                // WebSocketì´ë‚˜ ì˜¤ë””ì˜¤ ë…¹ìŒì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì¬ì‹œë„
                setTimeout(requestInitialResponse, 200);
              }
            };
            
            // ì˜¤ë””ì˜¤ ë…¹ìŒì´ ì‹œì‘ëœ í›„ì— ì´ˆê¸° ì‘ë‹µ ìš”ì²­ (ë” ê¸´ ë”œë ˆì´)
            setTimeout(requestInitialResponse, 1000);
          }

          // ì˜¤ë””ì˜¤ ì‘ë‹µ ì²˜ë¦¬ - ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì´ë²¤íŠ¸ íƒ€ì… í™•ì¸
          if (data.type === 'response.audio.delta') {
            if (data.delta) {
              console.log('ğŸ”Š [OpenAI] Audio delta received, length:', data.delta.length);
              const binaryString = atob(data.delta);
              const bytes = new Uint8Array(binaryString.length);
              for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
              }
              
              if (!audioStreamerRef.current) {
                audioStreamerRef.current = new AudioStreamer(await audioContext());
              }
              
              audioStreamerRef.current.addPCM16(bytes);
            } else {
              console.warn('âš ï¸ [OpenAI] response.audio.delta received but delta is empty');
            }
          }
          
          // ë‹¤ë¥¸ ì˜¤ë””ì˜¤ ì´ë²¤íŠ¸ íƒ€ì…ë“¤ë„ í™•ì¸
          if (data.type === 'response.output_audio.delta' || data.type === 'response.output_audio_delta') {
            console.log('ğŸ”Š [OpenAI] Alternative audio delta event received:', data.type);
            if (data.delta) {
              const binaryString = atob(data.delta);
              const bytes = new Uint8Array(binaryString.length);
              for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
              }
              
              if (!audioStreamerRef.current) {
                audioStreamerRef.current = new AudioStreamer(await audioContext());
              }
              
              audioStreamerRef.current.addPCM16(bytes);
            }
          }
          
          // ì˜¤ë””ì˜¤ ì‘ë‹µ ì‹œì‘
          if (data.type === 'response.audio_started' || data.type === 'response.output_audio.started') {
            console.log('ğŸµ [OpenAI] Audio response started');
          }
          
          // ì˜¤ë””ì˜¤ ì‘ë‹µ ì™„ë£Œ
          if (data.type === 'response.audio_done' || data.type === 'response.output_audio.done') {
            console.log('ğŸµ [OpenAI] Audio response done');
          }

          // ìŒì„± ì‹œì‘ ê°ì§€
          if (data.type === 'input_audio_buffer.speech_started') {
            console.log('ğŸ¤ [OpenAI] Speech started - VAD detected speech');
          }

          // ìŒì„± ì¤‘ì§€ ê°ì§€ - ì´ë•Œ ì‘ë‹µ ìƒì„± ì‹œì‘
          if (data.type === 'input_audio_buffer.speech_stopped') {
            console.log('ğŸ›‘ [OpenAI] Speech stopped - requesting response...');
            // ìŒì„±ì´ ì¤‘ì§€ë˜ë©´ response.createë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­
            setTimeout(() => {
              if (wsRef.current?.readyState === WebSocket.OPEN) {
                console.log('ğŸš€ [OpenAI] Requesting response after speech stopped...');
                try {
                  wsRef.current.send(JSON.stringify({
                    type: 'response.create'
                  }));
                } catch (error) {
                  console.error('âŒ [OpenAI] Failed to send response.create after speech stopped:', error);
                }
              }
            }, 100);
          }

          // ìŒì„± ì»¤ë°‹ ì™„ë£Œ - ì´ë•Œ ìë™ìœ¼ë¡œ ì‘ë‹µì´ ìƒì„±ë˜ì–´ì•¼ í•¨
          if (data.type === 'input_audio_buffer.committed') {
            console.log('âœ… [OpenAI] Audio buffer committed - should trigger auto response');
            // ì»¤ë°‹ í›„ ìë™ìœ¼ë¡œ ì‘ë‹µì´ ìƒì„±ë˜ì–´ì•¼ í•˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­
            setTimeout(() => {
              if (wsRef.current?.readyState === WebSocket.OPEN) {
                console.log('ğŸš€ [OpenAI] Requesting response after audio committed...');
                try {
                  wsRef.current.send(JSON.stringify({
                    type: 'response.create'
                  }));
                } catch (error) {
                  console.error('âŒ [OpenAI] Failed to send response.create after commit:', error);
                }
              }
            }, 200);
          }

          // í…ìŠ¤íŠ¸ ì‘ë‹µ ì²˜ë¦¬
          if (data.type === 'response.audio_transcript.delta' && data.delta) {
            triggerEvent('message', {
              type: 'transcription',
              text: data.delta
            });
          }

          // Tool call ì²˜ë¦¬
          if (data.type === 'response.function_call_arguments.done') {
            triggerEvent('toolcall', {
              functionCalls: [{
                name: data.name,
                args: JSON.parse(data.arguments)
              }]
            });
          }

          // ì‘ë‹µ ìƒì„± ì‹œì‘
          if (data.type === 'response.created') {
            console.log('ğŸ¬ [OpenAI] Response created - ì „ì²´ ë°ì´í„°:', JSON.stringify(data, null, 2));
            
            // response ê°ì²´ í™•ì¸
            if (data.response) {
              console.log('ğŸ“¦ [OpenAI] Response created object:', {
                id: data.response.id,
                status: data.response.status,
                modality: data.response.modality,
                output: data.response.output
              });
            }
          }

          // ì‘ë‹µ ìƒì„± ì˜¤ë¥˜
          if (data.type === 'response.error') {
            console.error('âŒ [OpenAI] Response error:', data.error);
            triggerEvent('error', {
              type: 'response_error',
              ...data.error
            });
            
            // ì¿¼í„° ì´ˆê³¼ ì—ëŸ¬ì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if (data.error?.code === 'insufficient_quota') {
              console.error('âŒ [OpenAI] API Quota Exceeded - Please check your OpenAI billing and plan');
            }
          }

          // ì‘ë‹µ ì™„ë£Œ - ëª¨ë“  ì‘ë‹µ ë°ì´í„° í™•ì¸
          if (data.type === 'response.done') {
            console.log('âœ… [OpenAI] Response done - ì „ì²´ ë°ì´í„°:', JSON.stringify(data, null, 2));
            
            // response ê°ì²´ í™•ì¸
            if (data.response) {
              const status = data.response.status;
              const statusDetails = data.response.status_details;
              
              // ì‘ë‹µ ì‹¤íŒ¨ í™•ì¸
              if (status === 'failed' || statusDetails?.type === 'failed') {
                const error = statusDetails?.error || data.response.error;
                console.error('âŒ [OpenAI] Response failed:', {
                  status,
                  error_type: error?.type,
                  error_code: error?.code,
                  error_message: error?.message
                });
                
                // ì—ëŸ¬ ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°
                triggerEvent('error', {
                  type: 'response_failed',
                  code: error?.code,
                  message: error?.message || 'Response failed'
                });
                
                // ì¿¼í„° ì´ˆê³¼ ì—ëŸ¬ì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                if (error?.code === 'insufficient_quota') {
                  console.error('âŒ [OpenAI] API Quota Exceeded - Please check your OpenAI billing and plan');
                  alert('OpenAI API ì¿¼í„°ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ê³„ì •ì˜ ê²°ì œ ì •ë³´ì™€ í”Œëœì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
                }
                
                return; // ì‹¤íŒ¨í•œ ì‘ë‹µì€ turncomplete íŠ¸ë¦¬ê±°í•˜ì§€ ì•ŠìŒ
              }
              
              console.log('ğŸ“¦ [OpenAI] Response object:', {
                id: data.response.id,
                status: data.response.status,
                output: data.response.output,
                has_audio: !!data.response.output?.find((o: any) => o.type === 'audio'),
                has_text: !!data.response.output?.find((o: any) => o.type === 'text'),
                output_types: data.response.output?.map((o: any) => o.type)
              });
            }
            
            // ì‘ë‹µ ì•„ì´í…œ í™•ì¸ (ì˜¤ë””ì˜¤ê°€ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€)
            if (data.item) {
              console.log('ğŸ“¦ [OpenAI] Response item:', {
                item_id: data.item.id,
                has_audio: !!data.item.audio,
                content: data.item.content?.slice(0, 2)
              });
            }
            
            triggerEvent('turncomplete', {});
          }
          
          // ì‘ë‹µì˜ ëª¨ë“  ë¶€ë¶„ ìˆ˜ì§‘
          if (data.type === 'response.output_item.added') {
            console.log('ğŸ“¦ [OpenAI] Response output item added:', {
              item_id: data.item?.id,
              item_type: data.item?.type,
              has_content: !!data.item?.content
            });
          }

          // ì—ëŸ¬ ì²˜ë¦¬
          if (data.type === 'error') {
            console.error('âŒ [OpenAI] Error:', data.error);
            triggerEvent('error', data.error);
          }
        } catch (error) {
          console.error('âŒ [OpenAI] Message parsing error:', error);
        }
      };

      wsRef.current.onerror = (error) => {
        console.error('âŒ [OpenAI] WebSocket error:', error);
        setConnected(false);
      };

      wsRef.current.onclose = () => {
        console.log('ğŸ”Œ [OpenAI] WebSocket closed');
        setConnected(false);
        sessionCreatedRef.current = false;
      };

      // ì˜¤ë””ì˜¤ ë…¹ìŒ ì‹œì‘
      if (!audioRecorderRef.current) {
        console.log('ğŸ¤ [OpenAI] Starting audio recorder...');
        audioRecorderRef.current = new AudioRecorder(24000);
        let audioChunkCount = 0;
        audioRecorderRef.current.on('data', (base64Audio: string) => {
          if (wsRef.current?.readyState === WebSocket.OPEN) {
            audioChunkCount++;
            if (audioChunkCount % 50 === 0) {
              console.log(`ğŸ¤ [OpenAI] Sent ${audioChunkCount} audio chunks`);
            }
            wsRef.current.send(JSON.stringify({
              type: 'input_audio_buffer.append',
              audio: base64Audio
            }));
          }
        });
        await audioRecorderRef.current.start();
        console.log('âœ… [OpenAI] Audio recorder started');
      }
    } catch (error) {
      console.error('âŒ [OpenAI] Connection error:', error);
      setConnected(false);
    } finally {
      connectingRef.current = false;
    }
  }, [config, model]);

  // ì—°ê²° í•´ì œ
  const disconnect = useCallback(async () => {
    console.log('ğŸ”Œ [OpenAI] Disconnecting...');
    
    if (audioRecorderRef.current) {
      audioRecorderRef.current.stop();
      audioRecorderRef.current = null;
    }

    if (audioStreamerRef.current) {
      audioStreamerRef.current.stop();
      audioStreamerRef.current = null;
    }

    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }

    setConnected(false);
    sessionCreatedRef.current = false;
    console.log('âœ… [OpenAI] Disconnected');
  }, []);

  // ë©”ì‹œì§€ ì „ì†¡
  const send = useCallback((parts: Array<{ text: string }>) => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      console.warn('âš ï¸ [OpenAI] WebSocket not ready');
      return;
    }

    const text = parts.map(p => p.text).join(' ');
    console.log('ğŸ“¤ [OpenAI] Sending message:', text.substring(0, 100));

    wsRef.current.send(JSON.stringify({
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'user',
        content: [{
          type: 'input_text',
          text: text
        }]
      }
    }));

    // Server VAD ëª¨ë“œì—ì„œëŠ” ì˜¤ë””ì˜¤ ì…ë ¥ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬
    // í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë„ Server VADê°€ ì²˜ë¦¬í•˜ë„ë¡ ëŒ€ê¸°
    console.log('âœ… [OpenAI] Message sent, Server VAD will handle response');
  }, []);

  // Tool ì‘ë‹µ ì „ì†¡
  const sendToolResponse = useCallback((response: any) => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;
    
    console.log('ğŸ”§ [OpenAI] Sending tool response:', response);
    wsRef.current.send(JSON.stringify({
      type: 'conversation.item.create',
      item: {
        type: 'function_call_output',
        call_id: response.id,
        output: JSON.stringify(response.result)
      }
    }));
  }, []);

  // ì´ˆê¸° ì‘ë‹µ ìƒì„±
  const createResponse = useCallback(() => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      console.warn('âš ï¸ [OpenAI] WebSocket not ready for response.create');
      return;
    }
    console.log('ğŸš€ [OpenAI] Creating response...');
    wsRef.current.send(JSON.stringify({
      type: 'response.create'
    }));
  }, []);

  // í´ë¼ì´ì–¸íŠ¸ ë˜í¼
  const clientWrapper = useMemo(() => ({
    send,
    sendToolResponse,
    createResponse,
    sendRealtimeInput: (chunks: Array<{ mimeType: string; data: string }>) => {
      if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;
      
      chunks.forEach((chunk) => {
        if (chunk.mimeType.includes('audio')) {
          wsRef.current?.send(JSON.stringify({
            type: 'input_audio_buffer.append',
            audio: chunk.data
          }));
        }
      });
    }
  }), [send, sendToolResponse, createResponse]);

  // Cleanup
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    client: clientWrapper,
    agent: null,
    setConfig,
    config,
    model,
    setModel,
    connected,
    setupComplete,
    connect,
    disconnect,
    volume,
    send,
    sendToolResponse,
    on,
    off
  };
}
